{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Large Language Model from Scratch in PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phXcjDRxKPZX"
      },
      "source": [
        "### Part I: Data Preparation and Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkQTkf_7Gs_Q"
      },
      "source": [
        "In this section we cover the data preparation and sampling to get our input data ready for the LLM. You can download our sample data from here: https://en.wikisource.org/wiki/The_Verdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfyfVfmBYm92",
        "outputId": "91d114ca-7c1a-4082-b211-e5d496398817"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of characters: 20479\n",
            "I HAD always thought\n"
          ]
        }
      ],
      "source": [
        "with open(\"sample_data/the-verdict.txt\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "print(f\"Total number of characters: {len(raw_text)}\")\n",
        "print(raw_text[:20]) # print the first 20 charaters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LqnpaG6HDEr"
      },
      "source": [
        "Next we tokenize and embed the input text for our LLM.\n",
        "- First we develop a simple tokenizer based on some sample text that we then apply to the main input text above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyw14TBYGfNM",
        "outputId": "c0c7ba36-a0f3-41f5-ec59-6d0176e3c6fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4649\n",
            "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "# Tokenize our input by splitting on whitespace and other characters\n",
        "# Then we strip whitespace from each item and then filer out any empty strings\n",
        "tokenized_raw_text = [item.strip() for item in re.split(r'([,.?_!\"()\\']|--|\\s)', raw_text) if item.strip()]\n",
        "print(len(tokenized_raw_text))\n",
        "print(tokenized_raw_text[:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiUyiOkvJOmX"
      },
      "source": [
        "Next we convert the text tokens into token Ids that can be processed via embedding layers later. We can then build a vocabulary that consists of all the unique tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7Zmc0M5JWKM",
        "outputId": "d5eee297-851f-4b04-8e42-1bb4ac6e91e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocab size: 1159\n"
          ]
        }
      ],
      "source": [
        "words = sorted(list(set(tokenized_raw_text)))\n",
        "vocab_size = len(words)\n",
        "print(f\"Vocab size: {vocab_size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjQfnSk0HoP3",
        "outputId": "c1b92bf8-e6ab-47f8-b0bb-59ded3164282"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('!', 0)\n",
            "('\"', 1)\n",
            "(\"'\", 2)\n",
            "('(', 3)\n",
            "(')', 4)\n",
            "(',', 5)\n",
            "('--', 6)\n",
            "('.', 7)\n",
            "(':', 8)\n",
            "(';', 9)\n",
            "('?', 10)\n",
            "('A', 11)\n",
            "('Ah', 12)\n",
            "('Among', 13)\n",
            "('And', 14)\n",
            "('Are', 15)\n",
            "('Arrt', 16)\n",
            "('As', 17)\n",
            "('At', 18)\n",
            "('Be', 19)\n",
            "('Begin', 20)\n",
            "('Burlington', 21)\n",
            "('But', 22)\n",
            "('By', 23)\n",
            "('Carlo', 24)\n",
            "('Carlo;', 25)\n",
            "('Chicago', 26)\n",
            "('Claude', 27)\n",
            "('Come', 28)\n",
            "('Croft', 29)\n",
            "('Destroyed', 30)\n",
            "('Devonshire', 31)\n",
            "('Don', 32)\n",
            "('Dubarry', 33)\n",
            "('Emperors', 34)\n",
            "('Florence', 35)\n",
            "('For', 36)\n",
            "('Gallery', 37)\n",
            "('Gideon', 38)\n",
            "('Gisburn', 39)\n",
            "('Gisburns', 40)\n",
            "('Grafton', 41)\n",
            "('Greek', 42)\n",
            "('Grindle', 43)\n",
            "('Grindle:', 44)\n",
            "('Grindles', 45)\n",
            "('HAD', 46)\n",
            "('Had', 47)\n",
            "('Hang', 48)\n",
            "('Has', 49)\n",
            "('He', 50)\n"
          ]
        }
      ],
      "source": [
        "vocabulary = {token:integer for integer, token in enumerate(words)}\n",
        "\n",
        "#Lets check the first 50 entries\n",
        "for i, item in enumerate(vocabulary.items()):\n",
        "    print(item)\n",
        "    if i == 50:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfI4Ji7MKW82"
      },
      "source": [
        "We can put these all together into our tokenizer class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lyJrFCNJ-Pj",
        "outputId": "352da894-8df9-4547-e7f8-dfb323c3f1aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 58, 2, 872, 1013, 615, 541, 763, 5, 1155, 608, 5, 1, 69, 7, 39, 873, 1136, 773, 812, 7]\n",
            "It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
          ]
        }
      ],
      "source": [
        "class TokenizerLayer:\n",
        "    def __init__(self, vocabulary):\n",
        "        self.token_to_int = vocabulary\n",
        "        self.int_to_token = {integer:token for token, integer in vocabulary.items()}\n",
        "\n",
        "    # The encode function turns text into token ids\n",
        "    def encode(self, text):\n",
        "        encoded_text = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
        "        encoded_text = [item.strip() for item in encoded_text if item.strip()]\n",
        "        return [self.token_to_int[token] for token in encoded_text]\n",
        "\n",
        "    # The decode function turns token ids back into text\n",
        "    def decode(self, ids):\n",
        "        text = \" \".join([self.int_to_token[i] for i in ids])\n",
        "        # Replace spaces before the specified punctuations\n",
        "        return re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "\n",
        "# Initialize and test tokenizer layer\n",
        "tokenizer = TokenizerLayer(vocabulary)\n",
        "print(tokenizer.encode(\"\"\"\"It's the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\"\"\"))\n",
        "print(tokenizer.decode(tokenizer.encode(\"\"\"It's the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\"\"\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYuGTtT9Na6D"
      },
      "source": [
        "Next we special tokens for unknown words and to mark end of text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "il5o_N4u_VLU"
      },
      "source": [
        "SPecial tokens include:\n",
        "\n",
        "[BOS] - Beginning of Sequence\n",
        "\n",
        "[EOS] - End of Sequence. This markds the end of a text, usually used to concatenate multiple unrelated texts e.g. two different documents, wikipedia articles, books etc.\n",
        "\n",
        "[PAD] - Padding: If we train an LLM with a batch size greater than 1, we may include multiple texts with different lenghts; with the padding token we pad the shorter texts to the longest length so that all texts have an equal lenght.\n",
        "\n",
        "[UNK] - denotes words not included in the vocabulary\n",
        "GPT2 only uses <|endoftext|> token for end of sequence and padding to reduce complexity which is analogous to [EOS].\n",
        "Instead of <UNK> token for out-of-vocabulary words, GPT-2 uses byte-pair encoding (BPE) tokenizer, which breaks down words into subword unis.\n",
        "For our application, we use <|endoftext|> tokens between two independent sources of text.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GNFTgprK2sX",
        "outputId": "e8f4b0e6-c89c-48af-d6c6-c91dbb840ec4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4649\n",
            "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was']\n"
          ]
        }
      ],
      "source": [
        "tokenized_raw_text = [item.strip() for item in re.split(r'([,.?_!\"()\\']|--|\\s)', raw_text) if item.strip()]\n",
        "all_tokens = sorted(list(set(tokenized_raw_text)))\n",
        "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
        "vocabulary = {token:integer for integer, token in enumerate(all_tokens)}\n",
        "tokenizer = TokenizerLayer(vocabulary)\n",
        "print(len(tokenized_raw_text))\n",
        "print(tokenized_raw_text[:20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDjWETO0B2Iq",
        "outputId": "31a9b699-1b45-4425-98e2-06c306f49372"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('younger', 1156)\n",
            "('your', 1157)\n",
            "('yourself', 1158)\n",
            "('<|endoftext|>', 1159)\n",
            "('<|unk|>', 1160)\n",
            "1161\n"
          ]
        }
      ],
      "source": [
        "for i, item in enumerate(list(vocabulary.items())[-5:]):\n",
        "    print(item)\n",
        "\n",
        "# Get the new length of our vocabulary\n",
        "print(len(vocabulary.items()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGubK3zwCMKh",
        "outputId": "ccdd9410-721d-41fb-c6ec-98bf4fd4ef9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 58, 2, 872, 1013, 615, 541, 763, 5, 1155, 608, 5, 1, 69, 7, 39, 873, 1136, 773, 812, 7]\n",
            "It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n",
            "[1, 101, 595, 119, 1160, 0, 1159, 113, 595, 1157, 1160, 1160]\n",
            "This is a <|unk|>! <|endoftext|> What is your <|unk|> <|unk|>\n"
          ]
        }
      ],
      "source": [
        "class TokenizerLayer:\n",
        "    def __init__(self, vocabulary):\n",
        "        self.token_to_int = vocabulary\n",
        "        self.int_to_token = {integer:token for token, integer in vocabulary.items()}\n",
        "\n",
        "    # The encode function turns text into token ids\n",
        "    def encode(self, text):\n",
        "        encoded_text = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
        "        encoded_text = [item.strip() for item in encoded_text if item.strip()]\n",
        "        encoded_text = [item if item in self.token_to_int else \"<|unk|>\" for item in encoded_text]\n",
        "        return [self.token_to_int[token] for token in encoded_text]\n",
        "\n",
        "    # The decode function turns token ids back into text\n",
        "    def decode(self, ids):\n",
        "        text = \" \".join([self.int_to_token[i] for i in ids])\n",
        "        # Replace spaces before the specified punctuations\n",
        "        return re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "\n",
        "# Initialize and test tokenizer layer\n",
        "tokenizer = TokenizerLayer(vocabulary)\n",
        "print(tokenizer.encode(\"\"\"\"It's the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\"\"\"))\n",
        "print(tokenizer.decode(tokenizer.encode(\"\"\"It's the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\"\"\")))\n",
        "\n",
        "print(tokenizer.encode(\"\"\"\"This is a test! <|endoftext|> What is your favourite movie\"\"\"))\n",
        "print(tokenizer.decode(tokenizer.encode(\"\"\"This is a test! <|endoftext|> What is your favourite movie\"\"\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFxrPqDvEdWh"
      },
      "source": [
        "#### Byte Pair Encoding (BPE)\n",
        "GPT-2 uses BPE as its tokenizer. This allows it to break down words that aren't in its predefined vocabulary into smaller subword units or even individual characters, enabling it to handle out-of-vocabulary words.\n",
        "\n",
        "For example, if GPT-2's vocabulary doesn't have the word \"unfamiliarword,\" it might tokenize it as [\"unfam\", \"iliar\", \"word\"] or some other subword breakdown, depending on its trained BPE merges\n",
        "\n",
        "Original BPE Tokenizer can be found here: https://github.com/openai/gpt-2/blob/master/src/encoder.py\n",
        "\n",
        "\n",
        "To use BPE tokenizer, we can use OpenAI's open-source tiktoken library which implements its core algorithms in Rust to improve computational performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LsORWqSaCMF9",
        "outputId": "f4597047-962e-41e1-c995-7a806e885a0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.6.0\n"
          ]
        }
      ],
      "source": [
        "# pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aoSdY7UmCL-X",
        "outputId": "fddd5ffd-e801-4b65-8db2-0f11e0897ee9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tiktoken version: 0.6.0\n"
          ]
        }
      ],
      "source": [
        "import tiktoken\n",
        "import importlib\n",
        "\n",
        "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7r1rDWCCLzN",
        "outputId": "cf9325be-7383-4de9-9a49-ed45eaeb2621"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[15496, 11, 428, 318, 257, 1332, 6827, 13, 220, 50256, 632, 338, 262, 938, 339, 13055, 11, 345, 760, 11]\n",
            "Hello, this is a test sentence. <|endoftext|> It's the last he painted, you know,\n"
          ]
        }
      ],
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "text = \"Hello, this is a test sentence from theouterspace. <|endoftext|> It's the last he painted, you know,\"\n",
        "token_ids = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "print(token_ids)\n",
        "\n",
        "# Re-construct the input text using the token_ids\n",
        "print(tokenizer.decode(token_ids))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "py-Ib8ObLmuF"
      },
      "source": [
        "BPE tokenizer breaks down the unknown words into subwords and individual characters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVRyPwVnPAr0"
      },
      "source": [
        "#### Data sampling with sliding window\n",
        "We train LLM to generate one word at a time, so we want to prepare the training data accordingly where the next word in a sequence represents the target to predict:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "Sb8xwjtBPurL",
        "outputId": "1b7b059e-ec82-45f4-de13-68018584c901"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<img src=\"https://drive.google.com/file/d/1-IpY_qgU0n704QJmoQYf8cAFIpeTuvTx/view?usp=sharing\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from IPython.display import Image\n",
        "Image(url=\"https://drive.google.com/file/d/1-IpY_qgU0n704QJmoQYf8cAFIpeTuvTx/view?usp=sharing\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhqd1wzpKWnc",
        "outputId": "32c8441d-0e72-4b66-bc0c-9e49770269e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5145\n"
          ]
        }
      ],
      "source": [
        "with open(\"sample_data/the-verdict.txt\", \"r\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "encoded_text = tokenizer.encode(raw_text)\n",
        "print(len(encoded_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jm6jFiwiT4H-"
      },
      "source": [
        "- For each ext chunk, we want inputs and targets\n",
        "- Since we want the model to predict the next word, the targets are the inputs shifted by one position to the right."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxMRD__ZOreA",
        "outputId": "42ead2ef-2663-4017-81f7-2dad5037900a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[40] -> 367\n",
            "[40, 367] -> 2885\n",
            "[40, 367, 2885] -> 1464\n",
            "[40, 367, 2885, 1464] -> 1807\n",
            "[40, 367, 2885, 1464, 1807] -> 3619\n"
          ]
        }
      ],
      "source": [
        "sample = encoded_text[:100]\n",
        "context_length = 5\n",
        "\n",
        "for i in range(1, context_length + 1):\n",
        "    context = sample[:i]\n",
        "    desired_target = sample[i]\n",
        "    print(context, \"->\", desired_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWUUkOIMSp6f",
        "outputId": "eba673ea-7b7e-4f99-e580-caa368b0b06a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I ->  H\n",
            "I H -> AD\n",
            "I HAD ->  always\n",
            "I HAD always ->  thought\n",
            "I HAD always thought ->  Jack\n"
          ]
        }
      ],
      "source": [
        "for i in range(1, context_length + 1):\n",
        "    context = sample[:i]\n",
        "    desired_target = sample[i]\n",
        "    print(tokenizer.decode(context), \"->\", tokenizer.decode([desired_target]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYCEBGdCVW33"
      },
      "source": [
        "### Data Loading\n",
        "Next we implement a simple data loader ha iterates over the input dataset and returns the inputs and target shifted by one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29KlipxnVMl7",
        "outputId": "bcf83afc-956e-4da1-addd-d6cd831390fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version: 2.2.1+cu121\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(\"PyTorch version:\", importlib.metadata.version(\"torch\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFH0ZbO1VrT0"
      },
      "source": [
        "- We use sliding window approach where we slide the window one word at a time (this is also called stride=1)\n",
        "- We create a dataset and dataloader object that extract chunks from the input text dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8O64kfclVlqh"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class LLMDataset(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        # Tokenize the entire text\n",
        "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "        # Iterate over the tokenized text\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            context = token_ids[i:i+max_length]\n",
        "            desired_target = token_ids[i+1:i+max_length+1]\n",
        "            self.input_ids.append(torch.tensor(context))\n",
        "            self.target_ids.append(torch.tensor(desired_target))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]\n",
        "\n",
        "\n",
        "def create_data_loader(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True):\n",
        "    # Initialize the tokenizer\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    # Create the dataset\n",
        "    dataset = LLMDataset(txt, tokenizer, max_length, stride)\n",
        "\n",
        "    # Create the data loader\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=drop_last)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dd1EbfqCcW_x",
        "outputId": "a53a24d2-1459-4a5e-c5b1-bae27b4ccf73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[tensor([[319, 616, 835, 284]]), tensor([[  616,   835,   284, 22489]])]\n"
          ]
        }
      ],
      "source": [
        "with open(\"sample_data/the-verdict.txt\", \"r\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "dataloader = create_data_loader(raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
        "data_iterator = iter(dataloader)\n",
        "batch = next(data_iterator)\n",
        "print(batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1j59QKxLctGN",
        "outputId": "4eed522a-c052-485f-fe88-5104baef9a84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[tensor([[ 11, 290,  11, 355]]), tensor([[ 290,   11,  355, 9074]])]\n"
          ]
        }
      ],
      "source": [
        "batch_2 = next(data_iterator)\n",
        "print(batch_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82frDa5Hc1Ci",
        "outputId": "0b6ebdec-834f-4f13-d5c0-bc07d75c83fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inputs:\n",
            " tensor([[41186, 39614,  1386,    11],\n",
            "        [  373,  3957,   588,   262],\n",
            "        [ 1169,  2994,   284,   943],\n",
            "        [ 7067, 29396, 18443, 12271],\n",
            "        [ 2666,   572,  1701,   198],\n",
            "        [ 3666, 13674,    11,  1201],\n",
            "        [ 1109,   815,   307,   900],\n",
            "        [  465,  5986,   438,  1169]])\n",
            "\n",
            "Targets:\n",
            " tensor([[39614,  1386,    11,   287],\n",
            "        [ 3957,   588,   262, 26394],\n",
            "        [ 2994,   284,   943, 17034],\n",
            "        [29396, 18443, 12271,   290],\n",
            "        [  572,  1701,   198,   198],\n",
            "        [13674,    11,  1201,   314],\n",
            "        [  815,   307,   900,   866],\n",
            "        [ 5986,   438,  1169,  3081]])\n"
          ]
        }
      ],
      "source": [
        "# Increse the stride to remove overlaps between the batches since more overlap could lead to increased overfitting\n",
        "dataloader = create_data_loader(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)\n",
        "print(\"Inputs:\\n\", inputs)\n",
        "print(\"\\nTargets:\\n\", targets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmEXu5EkeRjT"
      },
      "source": [
        "#### Creating token embeddings\n",
        "Next we embed the token in a continuous vector representation using an embedding layer. Usually the embedding layers are part of the LLM itself and are updated (trained) during model training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZYjz94MueG9F"
      },
      "outputs": [],
      "source": [
        "# Suppose we have the following four input examples with ids 5,1,3 and 2 after tokenization\n",
        "input_ids = torch.tensor([[5, 1, 3, 2]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-W5gJ3sXgB2O"
      },
      "source": [
        "For simplicity, suppose we have a small vocabulary of only 6 words and we want to create embeddings of size 3:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jb9jYH2igCg0",
        "outputId": "f66af148-6a90-42ff-9c57-ff7f49e55c19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 1.9269,  1.4873, -0.4974],\n",
            "        [ 0.4396, -0.7581,  1.0783],\n",
            "        [ 0.8008,  1.6806,  0.3559],\n",
            "        [-0.6866,  0.6105,  1.3347],\n",
            "        [-0.2316,  0.0418, -0.2516],\n",
            "        [ 0.8599, -0.3097, -0.3957]], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "vocab_size = 6\n",
        "embedding_size = 3\n",
        "\n",
        "torch.manual_seed(42)\n",
        "embedding_layer = torch.nn.Embedding(vocab_size, embedding_size)\n",
        "\n",
        "# This would result in a 6x3 weight matrix\n",
        "print(embedding_layer.weight)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dbw1VdkrgtjN"
      },
      "source": [
        "The embedding output for our example input tensor will look as follows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7buBerzgt_W",
        "outputId": "e7be15e1-75d1-4e48-c428-6517401f5cd1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[ 0.8599, -0.3097, -0.3957],\n",
              "         [ 0.4396, -0.7581,  1.0783],\n",
              "         [-0.6866,  0.6105,  1.3347],\n",
              "         [ 0.8008,  1.6806,  0.3559]]], grad_fn=<EmbeddingBackward0>)"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embedding_layer(input_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyjMbRgVhimE"
      },
      "source": [
        "#### Encoding Word Positions\n",
        "\n",
        "- Embedding layer convert Ids into identical vector representations regardless of where they are located in the input sequence.\n",
        "- Positional embeddings are combined with the token embedding vector to form the input embedding for a large language model\n",
        "- The BytePair encoder has a vocabulary size of 50,257\n",
        "- To encode the input token to a 256-dimensional representation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pyBRRYQIhDP5"
      },
      "outputs": [],
      "source": [
        "vocab_size = 50257\n",
        "embedding_dim = 256\n",
        "\n",
        "token_embedding_layer = torch.nn.Embedding(vocab_size, embedding_dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLY2m95pjDli"
      },
      "source": [
        "- if we sample data from the dataloader, we embed the tokens in each batch into a 256-dim vector\n",
        "- if we have a batch size of 8 with 4 tokens each, this will result in a 8x4x256 tensor:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4Dvz4gJi5Va",
        "outputId": "c33d04ab-4faa-44be-fc08-b2799439fc0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token Ids:\n",
            " tensor([[  273,  1807,   673,   750],\n",
            "        [21978, 44896,    11,   290],\n",
            "        [  991,  2045,   546,   329],\n",
            "        [ 7808,   607, 10927,  1108],\n",
            "        [ 3226,  1781,    11,  2769],\n",
            "        [   11,   644,   561,   339],\n",
            "        [  326,  9074,    13,   402],\n",
            "        [  373, 37895,   422,   428]])\n",
            "\n",
            "Inputs shape:\n",
            " torch.Size([8, 4])\n",
            "\n",
            "Embedding shape:\n",
            " torch.Size([8, 4, 256])\n"
          ]
        }
      ],
      "source": [
        "max_length = 4\n",
        "dataloader = create_data_loader(raw_text, batch_size=8, max_length=max_length, stride=max_length, shuffle=False)\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)\n",
        "print(\"Token Ids:\\n\", inputs)\n",
        "print(\"\\nInputs shape:\\n\", inputs.shape)\n",
        "print(\"\\nEmbedding shape:\\n\", token_embedding_layer(inputs).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRsQkfuRkjeX"
      },
      "source": [
        "- GPT-2 uses absolute position enbeddings, so we simply create another embedding layer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLRhefG9kANJ",
        "outputId": "72082af9-126b-4720-c858-104a94418dc8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 256])\n"
          ]
        }
      ],
      "source": [
        "context_length = max_length\n",
        "pos_embedding_layer = torch.nn.Embedding(context_length, embedding_dim)\n",
        "\n",
        "position_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
        "print(position_embeddings.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwRL5fVFl1ID"
      },
      "source": [
        "- To create the input embeddings used in an LLM, we add the token and positional embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBtm6BzAls8J",
        "outputId": "c052d642-6fad-45f7-fedd-d760e867d2db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ],
      "source": [
        "input_embeddings = token_embedding_layer(inputs) + position_embeddings\n",
        "print(input_embeddings.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xov1CsSAmYBN"
      },
      "source": [
        "The illustration below shows the end-to-end preprocessing steps of input tokens to an LLM model."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
